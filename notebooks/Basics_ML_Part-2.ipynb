{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Machine Learning `part 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "<!-- MarkdownTOC autolink=true autoanchor=true bracket=round -->\n",
    "\n",
    "- [Realworld dataset](#realworld_dataset)\n",
    "- [Performance metrics in classification](#perf_metrics)\n",
    "- [Model selection](#model_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"realworld_dataset\"></a>\n",
    "# Realworld dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from basics.utils import reduce_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider here a more complex dataset, stemming from \"real-world\" data: the **covtype** dataset provided within sklearn. From the sklearn user guide (https://scikit-learn.org/stable/datasets/index.html#covtype-dataset): \n",
    "> The samples in this dataset correspond to 30×30m patches of forest in the US, collected for the task of predicting each patch’s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the dataset’s homepage. Some of the features are boolean indicators, while others are discrete or continuous measurements.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "  | Element | value |\n",
    "  |---------|---|\n",
    "  | Classes | 7 |\n",
    "  | Samples total |\t581012 |\n",
    "  | Dimensionality |\t54 |\n",
    "  | Features | int |\n",
    "  \n",
    " Example of instance: ![](https://archive.ics.uci.edu/ml/assets/MLimages/Large31.jpg)\n",
    " \n",
    "Dataset website: https://archive.ics.uci.edu/ml/datasets/Covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "dataset = datasets.fetch_covtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset.data[1001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(dataset.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset.data\n",
    "labels = dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select a subset of the dataset in order to be able to have immediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[features, labels] = reduce_dataset(features, labels, num_obs=1660)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"perf_metrics\"></a>\n",
    "# Performance metrics in classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already mentioned that accuracy is a widely metrics to assess the performance of a model. It counts the number of good predictions among all the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect accuracy on the dataset considered. We first split the dataset into a training set and a testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Good practice**: shuffle the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.arange(len(features.data))\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(indexes)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffled dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features[indexes]\n",
    "Y = labels[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split between **training set** and **testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X[:int(0.8 * len(X))]\n",
    "train_Y = Y[:int(0.8 * len(Y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = X[int(0.8 * len(X)):]\n",
    "test_Y = Y[int(0.8 * len(Y)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect total sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.shape)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a classifier of our choice (e.g. SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = clf.score(test_X, test_Y)\n",
    "print('accuracy =', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION: what does it mean?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to have a statistics on the number of good answers our classifier is able to predict, but if we want to understand its behaviour we need more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common metric is to inspect the confusion the classifier is making between classes, it's what we called confusion matrix: it is counting the number of times an instance from class `i` has been predicted as class `j` where `j` can be `i` or another one. \n",
    "\n",
    "<img src='./assets/confusion_matrix.png' style=\"width:20%\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the confusion matrix \"manually\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(test_Y)\n",
    "num_classes = len(classes)\n",
    "print(num_classes, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat = np.zeros((num_classes, num_classes))\n",
    "for obs_i in range(len(test_X)):\n",
    "    pred_Y_i = clf.predict([test_X[obs_i]])\n",
    "    confmat[test_Y[obs_i] - 1, pred_Y_i - 1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(confmat)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn provides a method for computing confusion matrices, see dedicated page: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat = confusion_matrix(test_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(confmat)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional important metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notion of true positives and co."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We differentiate between true positives, true negatives, false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions. Let consider the case where we have to classify ann image `I` as either a `cat`, a `dog`, or a `donkey`. We first consider the class `cat`:\n",
    "- **True Positives (TPs)**: we know that an image `I` belongs to class `cat`, and it has been rightly labelled `cat` by the classifier\n",
    "- **True Negatives (TNs)**: we know that an observation `I` does not belong to class `cat` (in our exemple, it can belong to class `dog` or `donkey`, we don't really care) and it has not been labelled `cat` by the classifier\n",
    "- **False Positives (TPs)**: we know that an observation `I` does not belong to class `cat` (it belongs to class `dog` or `donkey`), and it has been wrongly labelled `cat` by the classifier\n",
    "- **False Negatives (TNs)**: we know that an observation `I` belongs to class `cat` and it has not been labelled `cat` by the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these categories, we can compute two useful measures: **Precision** and **Recall**. By taking the exemple above:\n",
    "- Precision is the proportion of images rightly categorized as `cat` among all the instances categorized as a `cat`\n",
    "- Recall is the proportion of images rightly categorized as `cat` among all instances that should have been categorized as `cat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Wikipedia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/precisionrecall.png\" style=\"width:30%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A widely used measure taking into account Precision and Recall is the **F1 Score**:\n",
    "\n",
    "$$fscore = 2*\\frac{precision \\times recall}{precision + recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[precision, recall, _, _] = precision_recall_fscore_support(test_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[precision, recall, _, _] = precision_recall_fscore_support(test_Y, pred_Y, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscore = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the accuracy?...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalance dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we made a strong constraint on our datasets : **each class has the same number of observations!**. This is not realistic in real world cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect what happens in case of datasets with different number of instance per class. We take the original data, this time we reduce the whole dataset by keeping a certain percentage of the observations per class, and not a fixed number of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset.data\n",
    "labels = dataset.target\n",
    "[features, labels] = reduce_dataset(features, labels, reduce_by=98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we shuffle and build the training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.arange(len(features.data))\n",
    "random.shuffle(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features[indexes]\n",
    "Y = labels[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X[:int(0.8 * len(X))]\n",
    "train_Y = Y[:int(0.8 * len(Y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = X[int(0.8 * len(X)):]\n",
    "test_Y = Y[int(0.8 * len(Y)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect number of instances per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in np.unique(labels):\n",
    "    idx_train = np.where(train_Y == c)[0]\n",
    "    idx_test = np.where(test_Y == c)[0]\n",
    "    print('class', c, '\\t num. training obs', len(idx_train), ' | num. testing obs', len(idx_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = clf.score(test_X, test_Y)\n",
    "print('accuracy =', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS:** \n",
    "- Do we have a better classifier than before? \n",
    "- What does this score mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice: plot the confusion matrix and comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = clf.predict(test_X)\n",
    "confmat = confusion_matrix(test_Y, pred_Y)\n",
    "plt.imshow(confmat)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let try to inspect relatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat = np.float32(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(confmat)):\n",
    "    confmat[i,:] = confmat[i,:] / np.sum(confmat[i,:])\n",
    "plt.imshow(confmat)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is limited because it does not give insights on performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the fscore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[precision, recall, _, _] = precision_recall_fscore_support(test_Y, pred_Y, average='macro')\n",
    "fscore = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy:', accuracy)\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('fscore:', fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model_selection\"></a>\n",
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, we usually compare various models in order to pick the best one for a particular application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing model by varying parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on webpage: https://scikit-learn.org/stable/modules/neural_networks_supervised.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/exerice-icon.png\" style=\"width:80px; float:left;\"></img><div style=\"clear:left;\"></div>\n",
    "**EXERCICE:** find the best MLP model in terms of `learning_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of this course, for the sake of comparison, we compare classification accuracy for two classifiers: \n",
    "- LDA: Linear Discriminant Analysis (linear model)\n",
    "- kNN: k-Nearest Neigbours (non linear) \n",
    "- MLP: Multi-layer Perception (non linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/exerice-icon.png\" style=\"width:80px; float:left;\"></img><div style=\"clear:left;\"></div>\n",
    "**EXERCICE:** find the best classifer between `discriminant_analysis` (LDA), `neighbors` (kNN) and `MLPClassifier` (Neural Network), trained on `train_X, train_Y` and tested on `test_X, test_Y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection is usually done using cross-validation: a way to to create different splits of a dataset and perform several tests, one for each split. This gives a statistical estimate of the generalisability of a classifier, and can be considered as a good measure to compare models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to split a dataset. Sklearn has several methods for that (see [API](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)):\n",
    "\n",
    "Function | Description\n",
    "--- | ---\n",
    "`model_selection.KFold([n_splits, shuffle, ...])` | K-Folds cross-validator\n",
    "`model_selection.GroupKFold([n_splits])`\t| K-fold iterator variant with non-overlapping groups.\n",
    "`model_selection.StratifiedKFold([n_splits, ...])`\t| Stratified K-Folds cross-validator\n",
    "`model_selection.LeaveOneGroupOut()`\t| Leave One Group Out cross-validator\n",
    "`model_selection.LeavePGroupsOut(n_groups)`\t| Leave P Group(s) Out cross-validator\n",
    "`model_selection.LeaveOneOut()`\t| Leave-One-Out cross-validator\n",
    "`model_selection.LeavePOut(p)`\t| Leave-P-Out cross-validator\n",
    "`model_selection.ShuffleSplit([n_splits, ...])`\t| Random permutation cross-validator\n",
    "`model_selection.GroupShuffleSplit([...])`\t| Shuffle-Group(s)-Out cross-validation iterator\n",
    "`model_selection.StratifiedShuffleSplit([...])`\t| Stratified ShuffleSplit cross-validator\n",
    "`model_selection.PredefinedSplit(test_fold)`\t| Predefined split cross-validator\n",
    "`model_selection.TimeSeriesSplit([n_splits])`\t| Time Series cross-validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partition of the initial dataset is usually caleld a fold. In our example below, we will used the **stratified k-fold** which creates folds preserving the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's for instance declare a stratified spitting method with a number of splits equals to 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = StratifiedKFold( n_splits = 4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload data such having the same number of observations per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset.data\n",
    "labels = dataset.target\n",
    "[features, labels] = reduce_dataset(features, labels, num_obs=1660)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter.split(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, test_index = next(splitter.split(train_X, train_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_split = train_X[train_index]\n",
    "train_Y_split = train_Y[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_split = train_X[test_index]\n",
    "test_Y_split = train_Y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_Y_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(test_Y_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop on splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in splitter.split(train_X, train_Y):\n",
    "    # Do something \n",
    "    TODO = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tests = 0\n",
    "\n",
    "score1 = []\n",
    "score2 = []\n",
    "score3 = []\n",
    "\n",
    "splitter = StratifiedKFold( n_splits = 12 )\n",
    "\n",
    "for train_index, test_index in splitter.split(train_X, train_Y):\n",
    "    \n",
    "    print('Split')\n",
    "\n",
    "    # select training and testing datasets\n",
    "    train_X_split = train_X[train_index]\n",
    "    train_Y_split = train_Y[train_index]\n",
    "    test_X_split = train_X[test_index]\n",
    "    test_Y_split = train_Y[test_index]\n",
    "    \n",
    "    clf1 = LinearDiscriminantAnalysis()\n",
    "    clf2 = KNeighborsClassifier()\n",
    "    clf3 = MLPClassifier()\n",
    "\n",
    "    clf1.fit(train_X, train_Y)\n",
    "    clf2.fit(train_X, train_Y)\n",
    "    clf3.fit(train_X, train_Y)\n",
    "\n",
    "    s1 = clf1.score(test_X, test_Y)\n",
    "    s2 = clf2.score(test_X, test_Y)\n",
    "    s3 = clf3.score(test_X, test_Y)\n",
    "    \n",
    "    score1.append(s1)\n",
    "    score2.append(s2)\n",
    "    score3.append(s3)\n",
    "\n",
    "print('LDA:', np.mean(score1))\n",
    "print('kNN:', np.mean(score2))\n",
    "print('MLP:', np.mean(score3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "About this material: copyright Baptiste Caramiaux (write me for any questions or use of this material [email](mailto:baptiste.caramiaux@lri.fr))\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
