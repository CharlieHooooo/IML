{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Machine Learning `part 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<!-- MarkdownTOC autolink=true autoanchor=true bracket=round -->\n",
    "\n",
    "- [Machine Learning](#machinelearning)\n",
    "- [Machine learning in python](#ml_python)\n",
    "- [The Iris Dataset](#toyexample)\n",
    "- [Training a Classifier on the Iris Dataset](#toyex_training)\n",
    "- [Testing a Classifier on the Iris Dataset](#toyex_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"machinelearning\"></a>\n",
    "# Machine Learning\n",
    "\n",
    "Machine learning has become recently a hot topic present in scientific literature as well as in the general public literature. Roughly saying, machine learning is *way* for computer to learn from data, and consequently, has been  at the heart of artificial intelligence algorithms. \n",
    "\n",
    "ML is nowadays already in many of the products we use Today from recommendation systems able to provide personnalized recommendations for filems (Netflix), music (Apple Music) or more general products (Amazon). Machine learning is also widely used to filters our spam, to recognise voice in personnal assistants, etc. \n",
    "\n",
    "> A formal definition of **machine learning** (ML) can be: a set of methods that can **automatically detect patterns** in data, and then use the uncovered patterns to **predict future data** (*from Machine Learning: A Probabilistic Perspective (Murphy 2012)*)\n",
    "\n",
    "<a name=\"ml_types\"></a>\n",
    "## Types of learning\n",
    "\n",
    "What are the types of \"learning\"? In ML literature, there are several types of learning, the three main types are: supervised learning, unsupervised learning and reinforcement learning. \n",
    "\n",
    "In supervised learning, you have access to a dataset where each datapoint, considered as an *input*, has an associated known output, that can be discrete (like a *label*) or continuous. \n",
    "\n",
    "The goal of a supervised learning algorithm is to learn the relationship between the inputs and their associated outputs. In case the outputs are discrete (*labels*) we call the task **classification**, in case the outputs are continous, the task is called **regression**. \n",
    "\n",
    "<img src=\"assets/supervised_learning.png\" width=400px></img>\n",
    "\n",
    "\n",
    "In unsupervised learning, you have acess to a dataset where there are only *inputs* (datapoints without associated labels or some values). The goal of an unsupervised learning algorithm is then to infer from these data the underlying structure. This task is usually harder than supervised learning. \n",
    "\n",
    "<img src=\"assets/unsupervised_learning.png\" width=400px></img>\n",
    "\n",
    "Reinforcement learning is a different paradigm: the learning occurs through interaction with the environment. In reinforcement learning, an agent takes actions in an environment such as to maximize some cumulative rewards. Each action taken produces a reward that the agent takes into account in order to assess the \"quality\" of the action. \n",
    "\n",
    "\n",
    "<a name=\"ml_phases\"></a>\n",
    "## Phases\n",
    "\n",
    "In ML, we usually consider two phases: **learning** then **testing**. \n",
    "\n",
    "**Training** an algorithm in machine learning means detecting patterns in a dataset. \n",
    "\n",
    "**Testing** an algorithm means predicting future data, that is generalizing the uncovered trained patterns to new datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ml_python\"></a>\n",
    "# Machine Learning in Python\n",
    "\n",
    "At this stage of the course, you should have installed Python 3, Jupyter Notebook and all the useful lirbaries used in this course (like sklearn). If not, please refer to the README.md "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick introduction to jupyter notebook\n",
    "\n",
    "### Cell\n",
    "Cell in jupyter notebook is where we write code or text that can be executed (or interpreted). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries \n",
    "\n",
    "At this stage, you cannot do much except calling the basic python functions such as '+' or '-' (and others). To be able to use additional functions we use libraries. Libraries are collections of functions that can be called in python programs. In order to use the functions included in a library, you first have to `import` that library. For instance, we will import the library allowing for manipulating arrays (vectors, matrices, etc.). This library is called `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use functions within the numpy library, like creating a 2 x 3 matrix with zeros inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = numpy.zeros((2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise what `M` looks like, you can use the `print` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the libray can be imported with a name (usually a shorter name), called a *namespace*. Namespaces make a code more readable and allow to define functions with a same name but within different namespaces. Numpy is usually imported as `np`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.zeros((5, 4))\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning libraries that we will use:\n",
    "- scikit learn https://scikit-learn.org/stable/index.html\n",
    "- keras https://keras.io/\n",
    "\n",
    "More details below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data\n",
    "\n",
    "Plotting data can be done using the `matplotlib` library (there are other ways to do, but this one is the most common and convenient as it uses matlab-like syntax). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try out this by plotting a simple vector of random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.random.randn(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(M)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(M, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we plotted the points sequentially, sometimes it is useful to display their **distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(M)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/exerice-icon.png\" style=\"width:80px; float:left;\"></img><div style=\"clear:left;\"></div>\n",
    "**EXERCICE:**: Try with a **higher number** of radomnly generated points.\n",
    "\n",
    "What is this shape? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn: A machine learning library in python\n",
    "\n",
    "Sklearn (or scikit-learn) is one of the most used machine learning library in python. The library includes the most common classifiers: Support Vector Machine (SVM), k-Nearest Neighbour (kNN), Gaussian Mixture Models (GMM); the reduction dimension techniques, such as Principal Component Analysis (PCA), Independent Component Analysis (ICA); techniques to split datasets, to preprocess data and cross-validate models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if sklearn is installed:\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sklearn API**: http://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning:\n",
    "- Tensorflow (google)\n",
    "- Keras (google)\n",
    "- PyTorch\n",
    "- Caffe\n",
    "- ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toyexample\"></a>\n",
    "# The Iris dataset\n",
    "\n",
    "An exemple of a toy dataset (simpler than realworld dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toyex_description\"></a>\n",
    "## Dataset Description\n",
    "\n",
    "This dataset consists of 3 different types of irises’ (**Setosa**, **Versicolour**, and **Virginica**) given by their:\n",
    "* Sepal Length\n",
    "* Sepal Width\n",
    "* Petal Length \n",
    "* Petal Width.\n",
    "\n",
    "<div style=\"width:300px; margin-top:10px; margin-right:10px; float:left;\">![pic1](https://apps.rhs.org.uk/plantselectorimages/detail/RHS_PUB0001482_1050.JPG \"pic1\") **Iris Setosa**</div>\n",
    "<div style=\"width:263px; margin-top:10px; margin-right:10px; float:left;\">![pic1](https://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg \"pic1\") **Iris Versicolour**</div>\n",
    "<div style=\"width:242px; margin-top:10px; margin-right:10px; float:left;\">![pic1](https://upload.wikimedia.org/wikipedia/commons/9/9f/Iris_virginica.jpg \"pic1\") **Iris Virginica**</div>\n",
    "<div style=\"clear: left;\"></div>\n",
    "\n",
    "The classes are encoded as integers: \n",
    "* Setosa = **0**\n",
    "* Versicolour = **1**\n",
    "* Virginica = **2**\n",
    "\n",
    "Rows are the samples and the columns the feature dimensions (Sepal Length, Sepal Width, Petal Length and Petal Width).\n",
    "```\n",
    "sample 1: [ 5.1,  3.5,  1.4,  0.2] \n",
    "sample 2: [ 4.9,  3. ,  1.4,  0.2] \n",
    "sample 3: [ 4.7,  3.2,  1.3,  0.2] \n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toyex_visualising\"></a>\n",
    "## Exploring the dataset\n",
    "\n",
    "### Loading the data\n",
    "\n",
    "The iris dataset is included in the `sklearn` library as a benchmark for the algorithms implemented. The iris dataset is part of the `datasets` python classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned object `iris` includes the data (4-dimensional data points) and the labels (integer associated to each datapoint within the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = iris.data\n",
    "labels = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the first 5 samples in the dataset\n",
    "features[0:5,0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GOAL** to learn the relationship between each 4-dimensional vector (for instance `[4.9, 3. , 1.4]`) and the associated label (for instance `0`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple statistics\n",
    "\n",
    "Before starting to train algorithms on a given dataset, it is good practice to \"visualise\" the dataset as much as you can. \n",
    "\n",
    "This has several benefits: \n",
    "- to better understand the nature of the data, \n",
    "- to better understand the complexity of the data, \n",
    "- to better scope the class of model would do a good job on these data, \n",
    "- and to better understand model's results. \n",
    "\n",
    "By \"visualising\" a dataset, I mean having different metrics that can help these understandings. The simplest way is through simple statistics: computing the means and standard deviations of the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2.1 + 3.4 + 4.3) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt( ((2.1 - 3.26)**2 + (3.4 - 3.26)**2 + (4.3 - 3.26)**2 ) / 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, this can be computed with `numpy` functions: `np.mean` for computing a mean over one axis of the data, and `np.std` to compute the standard deviation over one axis (the variance could also be considered using `np.var`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [2.1, 3.4, 4.3]\n",
    "print(np.mean(v))\n",
    "print(np.std(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the mean and standard deviation of the Iris features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(features, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This give a **very** rough and insufficient information about the dataset. The next step is to inspect the means and standard deviations of the data, computed per class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/exerice-icon.png\" style=\"width:80px; float:left;\"></img><div style=\"clear:left;\"></div>\n",
    "**EXERCICE:** Compute means and standard deviation of the features for each class. Store in `means` and `stds` lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#figure(figsize=(16,6))\n",
    "#for c in classes:\n",
    "#    plt.subplot(1,3,c+1)\n",
    "#    plt.bar([0,1,2,3], means[c], yerr=stds[c])\n",
    "#    plt.ylim([0., 7.])\n",
    "#    plt.title('Class label ' + str(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature dimension is **4**, which makes it hard to visualize in a simple 2-d plot. There are several ways to reduce the dimensions of a dataset. This procedure is simply called *dimension reduction*. I won't detail these techniques here, it is beyond the scope of this lecture. What we will do instead is to select manually two dimensions among the 4 in order to plot the data on a 2-d graph. \n",
    "\n",
    "We start by selecting the two first dimensions and visualize them in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = features[:,0] # sepal length\n",
    "data_y = features[:,1] # sepal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['image.cmap'] = 'Set1'\n",
    "cmap = matplotlib.cm.get_cmap('Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_x, data_y, c=cmap(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: graphs always need legends and axis labelling!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basics.utils import get_iris_data\n",
    "features_c0, _, _ = get_iris_data(classes=[0])\n",
    "features_c1, _, _ = get_iris_data(classes=[1])\n",
    "features_c2, _, _ = get_iris_data(classes=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1 = 0\n",
    "dim2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = features_c0[:,dim1] # sepal length\n",
    "data_y = features_c0[:,dim2] # sepal width\n",
    "scatter(data_x, data_y, c=cmap(0), label='class 1')\n",
    "\n",
    "data_x = features_c1[:,dim1] # sepal length\n",
    "data_y = features_c1[:,dim2] # sepal width\n",
    "scatter(data_x, data_y, c=cmap(1), label='class 2')\n",
    "\n",
    "data_x = features_c2[:,dim1] # sepal length\n",
    "data_y = features_c2[:,dim2] # sepal width\n",
    "scatter(data_x, data_y, c=cmap(2), label='class 3')\n",
    "\n",
    "xlabel(\"Dimension 1: Sepal Length\")\n",
    "ylabel(\"Dimension 2: Sepal Width\")\n",
    "legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**: \n",
    "- What can we say on the data based on this graph?\n",
    "- Can we find a better representation space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/exerice-icon.png\" style=\"width:80px; float:left;\"></img><div style=\"clear:left;\"></div>\n",
    "**EXERCICE:** Visualise other dimensions of the dataset. Try to find the best visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we could find an heuristics to classify new datapoints as being in class 1, 2 or 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring by hand may be good to understand the data but it is too tedious to find the best criteria highlighting patterns in the data. That's why **Machine Learning** exists: finding underlying patterns and being able to generalise, that is make prediction, based on these patterns, on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toyex_training\"></a>\n",
    "# Training a Classifier on the Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As use case, we will explore classifier training with the **Support Vector Machine (SVM)**. The support vector machine, in its simplest version, is a **linear discriminant model** which means that the technique tries to discriminate between classes using lieanr function (basically lines in 2D, planes in 3D, hyperplanes otherwise). \n",
    "\n",
    "Another way to say it is that the **decision boundaries** are linear. \n",
    "\n",
    "Useful readings (although a bit techncial) to know more about SVM:\n",
    "* C Cortes, V Vapnik. Support-vector networks. _Machine learning_ 20 (3), 273-297, 1995\n",
    "* B Schölkopf, AJ Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. *MIT press*, 2002\n",
    "\n",
    "SVM in sklearn can be imported simply using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a new SVM instance called `classifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the linear version (simpler) of support vector machine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/clf_lin_nonlin.png' style='width:70%'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: classification is a **supervised learning** task, meaning that it learns the function mapping feature samples to known labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in np.random.randint(len(labels), size=10):\n",
    "    print(features[n,:], '\\t==> ', labels[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function: `fit()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `scikit-learn`, any trainable method has a function called `fit()` which takes a set of features and a set of corresponding labels as arguments, and trains the model:\n",
    "* `fit` is the generic function to train any methods in `sklearn`\n",
    "* for supervised methods, `fit` accepts two arguments: the feature data and their labels, that is `fit(X_train, y_train)`\n",
    "* for unsupervised methods, `fit` accepts only one argument: the feature data, that is `fit(X_train)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(features, labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the two dimensions visualized previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = features[:,2] # petal length\n",
    "data_y = features[:,3] # petal width\n",
    "scatter(data_x, data_y, c=cmap(labels))\n",
    "\n",
    "classifier.fit(features[:,2:], labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding *training* in SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding training procedure in machine learning starts by understanding the **decision boundary** which is the set of borders delimiting regions in the feature space associated to each labels. Let's take the two last dimensions of the iris data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scatter(features[:,2], features[:,3], c=cmap(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider only two classes given by the <span style=\"color:#DD0000;\">**RED**</span> and <span style=\"color:#0000DD;\">**BLUE**</span> colours (class 0 and 1 respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basics.utils import get_iris_data\n",
    "features, labels, colors = get_iris_data(classes=[0,1])\n",
    "print(colors.shape, features.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = features[:, 2:]\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X_train[:,0], X_train[:,1], color=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: what is the best decision boundary between classes 0 and 1?**\n",
    "\n",
    "Linear models, such as SVM, consider linear decision boundaries. In a 2-dimensional space, a linear decision boundary is a **line**. A line can be defined by 2 parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = -0.1\n",
    "intercept = 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the corresponding line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_x = np.linspace(1,5)\n",
    "boundary_y = slope * boundary_x + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X_train[:,0], X_train[:,1], color=colors)\n",
    "plot(boundary_x, boundary_y, '-k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Is that good enough?**\n",
    "\n",
    "Let's imagine that we have a new point coming in with data `(3.5, 0.7)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X_train[:,0], X_train[:,1], c=colors)\n",
    "plot(boundary_x, boundary_y, '-k')\n",
    "scatter(3.5, 0.7, c='green', s=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Which class does the blue point belong to? \n",
    "\n",
    "Let's try other values for the slope and the intercept..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = -1.0\n",
    "intercept = 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boundary_x = np.linspace(1,4)\n",
    "boundary_y = slope * boundary_x + intercept\n",
    "\n",
    "scatter(X_train[:,0], X_train[:,1], c=colors)\n",
    "plot(boundary_x, boundary_y, '-k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> Looks better...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About **training** in SVM:\n",
    "* means finding the **best parameters** wrt the set of samples\n",
    "* can often be understood as an **OPTIMIZATION** problem (i.e. finding a decision boundary such as miminzing a certain **cost function**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(features[:,2:], labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = classifier.coef_[0]\n",
    "slope = -coefs[0] / coefs[1]\n",
    "intercept = classifier.intercept_[0] / coefs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_x = np.linspace(1.5,3.5)\n",
    "boundary_y = slope * boundary_x - intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X_train[:,0], X_train[:,1], c=colors)\n",
    "plot(boundary_x, boundary_y, '-k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/exerice-icon.png\" style=\"width:80px; float:left;\"></img><div style=\"clear:left;\"></div>\n",
    "**EXERCICE:** Train a SVM classifier on the 4-dimensional feature data. What does it change? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising boundaries as regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of visualising the decision boundary, it is sometimes clearer to highlight regions of each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_x = np.linspace(0.75, 5.3, 300)\n",
    "mesh_y = np.linspace(-0.8, 2.5, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(features[:,2:], labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_area = np.zeros((len(mesh_x), len(mesh_y)))\n",
    "for i, x in enumerate(mesh_x):\n",
    "    for j, y in enumerate(mesh_y):\n",
    "        data_point = np.array([x, y]).reshape(1, 2)\n",
    "        decision_area[i,j] = classifier.predict(data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcolormesh(mesh_x, mesh_y, decision_area.T, alpha=0.1)\n",
    "scatter(X_train[:,0], X_train[:,1], c=y_train, edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: Why is it useful? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Linked to the goal of machine learning in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect more complex decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, colors = get_iris_data(classes=[0,1])\n",
    "X_train = features[:, 2:]\n",
    "y_train = labels\n",
    "classifier = svm.SVC(kernel='rbf') # <= NON-LINEAR \n",
    "classifier.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decision_area = np.zeros((len(mesh_x), len(mesh_y)))\n",
    "for i, x in enumerate(mesh_x):\n",
    "    for j, y in enumerate(mesh_y):\n",
    "        data_point = np.array([x, y]).reshape(1, 2)\n",
    "        decision_area[i,j] = classifier.predict(data_point)\n",
    "        \n",
    "pcolormesh(mesh_x, mesh_y, decision_area.T, alpha=0.1)\n",
    "scatter(X_train[:,0], X_train[:,1], c=y_train, edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising region boundaries of another type of classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider *kNN*: **k-Nearest Neighbour** (in sklearn: `neighbors.KNeighborsClassifier()`)\n",
    "\n",
    "If you don't know kNN, this classification algorithm assigns a label to a new input vector from a majority vote of its k nearest neighbors:\n",
    "\n",
    "![kNN](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/220px-KnnClassification.svg.png \"kNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn page: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, colors = get_iris_data(classes=[0,1])\n",
    "X_train = features[:, 2:]\n",
    "y_train = labels\n",
    "classifier = KNeighborsClassifier(n_neighbors=1) # <= NON-LINEAR \n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_area = np.zeros((len(mesh_x), len(mesh_y)))\n",
    "for i, x in enumerate(mesh_x):\n",
    "    for j, y in enumerate(mesh_y):\n",
    "        data_point = np.array([x, y]).reshape(1, 2)\n",
    "        decision_area[i,j] = classifier.predict(data_point)\n",
    "        \n",
    "pcolormesh(mesh_x, mesh_y, decision_area.T, alpha=0.1)\n",
    "scatter(X_train[:,0], X_train[:,1], c=y_train, edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, colors = get_iris_data(classes=[1,2])\n",
    "X_train = features[:, :2]\n",
    "y_train = labels\n",
    "classifier = KNeighborsClassifier(n_neighbors=1) # <= NON-LINEAR \n",
    "classifier.fit(X_train, y_train)\n",
    "mesh_x = np.linspace(4.0, 8.0, 100)\n",
    "mesh_y = np.linspace(2.0, 5.0, 100)\n",
    "decision_area = np.zeros((len(mesh_x), len(mesh_y)))\n",
    "for i, x in enumerate(mesh_x):\n",
    "    for j, y in enumerate(mesh_y):\n",
    "        data_point = np.array([x, y]).reshape(1, 2)\n",
    "        decision_area[i,j] = classifier.predict(data_point)\n",
    "pcolormesh(mesh_x, mesh_y, decision_area.T, alpha=0.1)\n",
    "scatter(X_train[:,0], X_train[:,1], c=y_train, edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toyex_multiclass\"></a>\n",
    "## Multiclass Problem\n",
    "\n",
    "Until now we have only seen the special case of discriminating between two classes: 0 or 1. But most of the real-world problems are actually dealing with multiple classes. For instance, in self-driving car, the car needs to discriminate between a human crossing the street, a dog crossing the street, a sign, another car, a truck, a bike, etc... The number of classes can be very high. \n",
    "\n",
    "Here, within our toy example, we will simply add the left-aside class and examine how it affects the decision boundarues as visualised before. Note that we are still using the two last dimensions of the data for the sake of understanding and visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, colors = get_iris_data(classes=[0, 1, 2])\n",
    "X_train = features[:, 2:]\n",
    "y_train = labels\n",
    "classifier = svm.SVC(kernel='linear')\n",
    "classifier.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with more than one class, SVM finds decision boundaries between pair of classes:\n",
    "* Class 1 vs. Class 2\n",
    "* Class 1 vs. Class 3\n",
    "* Class 2 vs. Class 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_x = np.linspace(0.0, 8.0, 100)\n",
    "mesh_y = np.linspace(0.0, 5.0, 100)\n",
    "decision_area = np.zeros((len(mesh_x), len(mesh_y)))\n",
    "for i, x in enumerate(mesh_x):\n",
    "    for j, y in enumerate(mesh_y):\n",
    "        data_point = np.array([x, y]).reshape(1, 2)\n",
    "        decision_area[i,j] = classifier.predict(data_point)\n",
    "pcolormesh(mesh_x, mesh_y, decision_area.T, alpha=0.1)\n",
    "scatter(X_train[:,0], X_train[:,1], c=y_train, edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/exerice-icon.png\" style=\"width:80px; float:left;\"></img><div style=\"clear:left;\"></div>\n",
    "**EXERCICE:** What are the decision regions for a `1-NN` in the of the multiclass problem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toyex_testing\"></a>\n",
    "# Testing a Classifier on the Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing a classifier is to inspect if the classifier performs a good prediction (estimated the label/class) on data that **are not included** in the dataset used to train it. \n",
    "\n",
    "The general objective of ML is indeed to test the **generalizability** of the trained algorithm. \n",
    "\n",
    "Algorithms can then be compared based on their prediction capabilities. Prediction capability is usually reported as accuracy percentage: what is the percentage of the testing dataset that has been correctly classified (predicted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test a classifier, we then need to split the whole dataset into a training dataset on which the classifier will be trained and a testing dataset on which we will test the prediciton capabilities. \n",
    "\n",
    "<img src='./assets/crossval_split_training_testing.png' style=\"width:40%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option is to shuffle the dataset and select 20% for testing and 80% for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: Why shuffling? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shuffle the dataset, let's first get indexes to browse the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, _ = get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the labels the **exact same way**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = features[:int(0.8 * len(features)), :]\n",
    "train_Y = labels[:int(0.8 * len(labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = features[int(0.8 * len(features)):, :]\n",
    "test_Y = labels[int(0.8 * len(labels)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test classification by computing the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init SVM classifier\n",
    "classifier = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train VM classifier\n",
    "classifier.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test SVM classifier and store output\n",
    "pred_Y = classifier.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of errors of our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_errors = 0\n",
    "for i,yi in enumerate(pred_Y):\n",
    "    if (yi != test_Y[i]):\n",
    "        num_errors += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance in percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "( 1.0 - num_errors/len(pred_Y) ) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the classification accuracy in percentage on the current set of training/testing datasets. The **worst classifier** would have returned 50% (_chance level_). In sklearn we can directly compute the score with the `score()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn function\n",
    "score = classifier.score(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Can we say that this classifier has 93.3% accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess generalizability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested only on one split, what if the split leads to particularly well disciminated training dataset but not testing dataset. Or the contrary... The idea is to consider more than one split, to compute the score on each split and then compute the average (and the variance of the scores) which gives a more robust way (statistically) to assess a classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/exerice-icon.png\" style=\"width:80px; float:left;\"></img><div style=\"clear:left;\"></div>\n",
    "**EXERCICE:** perform `N` (e.g. 20) varying the splits and inspect the robustness of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(all_scores)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.std(all_scores)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_scores, '--o')\n",
    "plt.ylim([0.9, 1.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "About this material: copyright Baptiste Caramiaux (write me for any questions or use of this material [email](mailto:baptiste.caramiaux@lri.fr))\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
